package protocol

import (
	"fmt"
	"strconv"
	"strings"
)

type TokenType string

const (
	numberToken TokenType = "numberToken"
	// spaceToken   TokenType = "spaceToken"
	eofToken     TokenType = "eofToken"
	plusToken    TokenType = "plusToken"
	dollarToken  TokenType = "dolarToken"
	hyphenToken  TokenType = "hyphenToken"
	colonToken   TokenType = "colonToken"
	literalToken TokenType = "literalToken"
	starToken    TokenType = "starToken"
	CLRFToken    TokenType = "CLRFToken"
)

type TokenVal interface{}

type Token struct {
	tokenType TokenType
	val       TokenVal
	rawVal    string
}

type Lexar struct {
	input string
	index int
}

type ParseErrorType string

const (
	ParseErrorMissingChunk ParseErrorType = "ParseErrorMissingChunk"
	ParseErrorInvalidData  ParseErrorType = "ParseErrorInvalidData"
)

type ParseError struct {
	msg       string
	errorType ParseErrorType
}

func (e ParseError) Error() string {
	return e.msg
}

func NewMissingChunkError(msg string) *ParseError {
	return &ParseError{msg: msg, errorType: ParseErrorMissingChunk}
}

func NewInvalidDataError(msg string) *ParseError {
	return &ParseError{msg: msg, errorType: ParseErrorInvalidData}
}

func (l Lexar) peek() byte {
	if l.eof() {
		return '$'
	}

	return l.input[l.index]
}

func (l *Lexar) next() byte {
	l.index++

	return l.peek()
}

func (l Lexar) eof() bool {
	return l.index >= len(l.input)
}

func isNumeric(char byte) bool {
	return char >= '0' && char <= '9'
}

func (l *Lexar) getNumber() string {
	char := l.peek()
	numberString := ""
	for isNumeric(char) {
		numberString += string(char)
		char = l.next()
	}

	return numberString
}

func (l *Lexar) readNumberToken() (Token, error) {
	isMinus := false
	if l.peek() == '-' {
		isMinus = true
		l.next()
	}

	numberString := l.getNumber()
	number, err := strconv.Atoi(numberString)

	if isMinus {
		numberString = string('-') + numberString
		number = number * -1
	}

	if err != nil {
		return Token{}, fmt.Errorf("error parsing value: %v to int", numberString)
	}
	return Token{tokenType: numberToken, val: number, rawVal: numberString}, nil
}

func isAlpha(char byte) bool {
	return (char >= 'a' && char <= 'z') || (char >= 'A' && char <= 'Z')
}

func isAlphaNumerical(char byte) bool {
	return isAlpha(char) || isNumeric(char)
}

func isUpperCase(char byte) bool {
	return char >= 'A' && char <= 'Z'
}

func isWordUpperCase(data string) bool {
	for _, char := range data {
		if !(isUpperCase(byte(char))) {
			return false
		}
	}

	return true
}

func (l *Lexar) readLiteralString() string {
	char := l.peek()
	output := ""

	for isAlphaNumerical(char) || char == ' ' || char == '-' || char == '*' || char == '+' || char == '_' || char == '.' ||
		char == '\'' || char == '|' || char == '(' || char == ')' || char == '/' || char == ':' || char == '>' {
		output += string(char)
		char = l.next()
	}
	return output
}

func (l *Lexar) parseDolarPattern() ([]Token, error) {
	tokens := []Token{}
	tokens = append(tokens, Token{tokenType: dollarToken, rawVal: "$"})
	l.next()
	numberToken, err := l.readNumberToken()
	if err != nil {
		return nil, err
	}
	tokens = append(tokens, numberToken)
	clrfToken, err := l.parseCLRFPattern()
	if err != nil {
		return nil, err
	}
	tokens = append(tokens, clrfToken...)
	literal := l.readStringLitteral()
	tokens = append(tokens, literal...)
	return tokens, nil
}

func (l *Lexar) parseStarPattern() ([]Token, error) {
	tokens := []Token{}
	tokens = append(tokens, Token{tokenType: starToken, rawVal: "*"})
	l.next()
	numberToken, err := l.readNumberToken()
	if err != nil {
		return nil, err
	}
	tokens = append(tokens, numberToken)
	return tokens, nil
}

func (l *Lexar) parseCLRFPattern() ([]Token, error) {
	tokens := []Token{}
	l.next()
	if l.eof() {
		return tokens, nil
	}
	if l.peek() != '\n' {
		return nil, fmt.Errorf("error while reading csrf, expected '\n' got: %v", l.peek())
	}

	tokens = append(tokens, Token{tokenType: CLRFToken, rawVal: "\r\n"})
	l.next()

	return tokens, nil
}

func (l *Lexar) parseNumberPattern() ([]Token, error) {
	tokens := []Token{}
	tokens = append(tokens, Token{tokenType: colonToken, rawVal: ":"})
	l.next()
	numberString := l.getNumber()

	number, err := strconv.Atoi(numberString)

	if err != nil {
		return tokens, err
	}

	tokens = append(tokens, Token{tokenType: numberToken, val: number, rawVal: numberString})

	return tokens, nil
}

func (l *Lexar) readStringLitteral() []Token {
	literal := l.readLiteralString()

	return []Token{{tokenType: literalToken, val: literal, rawVal: literal}}
}

func (l *Lexar) parseRespData() ([]Token, error) {
	tokens := []Token{}
	char := l.peek()
	var currTokens []Token
	var err error
	switch char {
	case ':':
		currTokens, err = l.parseNumberPattern()
	// case ' ':
	// 	currTokens = []Token{{tokenType: spaceToken, rawVal: " "}}
	// 	l.next()
	case '+':
		currTokens = []Token{{tokenType: plusToken, rawVal: "+"}}
		l.next()
	case '\r':
		currTokens, err = l.parseCLRFPattern()
	case '$':
		currTokens, err = l.parseDolarPattern()
	case '*':
		currTokens, err = l.parseStarPattern()
	case '-':
		currTokens = []Token{{tokenType: hyphenToken, rawVal: "-"}}
		l.next()
	default:
		currTokens = l.readStringLitteral()
	}
	if err != nil {
		return tokens, err
	}
	tokens = append(tokens, currTokens...)

	return tokens, nil
}

type LexarResult struct {
	Tokens       []Token
	UnparsedData string
	err          error
}

func (l *Lexar) parse() LexarResult {
	resp := []Token{}
	for !l.eof() {
		currentIndex := l.index
		tokens, err := l.parseRespData()

		if err != nil {
			return LexarResult{
				Tokens:       tokens,
				UnparsedData: l.input[currentIndex:],
				err:          err,
			}
		}

		resp = append(resp, tokens...)

	}

	return LexarResult{
		err:          nil,
		UnparsedData: "",
		Tokens:       resp,
	}
}

func NewLexar(input string) *Lexar {
	return &Lexar{
		input: input,
		index: 0,
	}
}

type Parser struct {
	tokens           []Token
	unconsumedTokens []Token
	index            int
}

func (p *Parser) eof() bool {
	return len(p.tokens) <= p.index
}
func (p *Parser) peek() Token {
	if p.eof() {
		return Token{tokenType: eofToken, val: "", rawVal: ""}
	}
	return p.tokens[p.index]
}

func (p *Parser) next() Token {
	p.index++

	return p.peek()
}

func (p *Parser) skipWhiteSpaces() {
	for p.peek().tokenType == CLRFToken {
		p.next()
	}
}

func (p *Parser) expect(tokenType TokenType) *ParseError {
	if p.eof() {
		return NewMissingChunkError(fmt.Sprintf("data ended when looking for %v", tokenType))
	}
	for p.peek().tokenType != tokenType {
		return NewInvalidDataError(fmt.Sprintf("expected token type: %v, got: %v", tokenType, p.peek().tokenType))
	}

	return nil
}

type ParseResult struct {
	err     error
	records []ParseResultRecord
}

type ASTNode interface{}

type ASTNumber struct {
	Val int
}

type ASTSimpleString struct {
	Val string
}

type ASTBulkString struct {
	Val string
}

type ASTSimpleError struct {
	ErrType string
	Msg     string
}

type ASTArray struct {
	Values []ASTNode
}

type ParseResultRecord struct {
	AstNode  ASTNode
	RawInput string
}

func (p *Parser) getRawInput(start int, end int) string {
	raw := ""
	for i := start; i <= end; i++ {
		raw += p.tokens[i].rawVal
	}

	return raw
}

func (p *Parser) parseStream(input string) ParseResult {
	p.index = 0
	lexar := NewLexar(input)
	result := lexar.parse()
	if result.err != nil {
		//inavalid data
		return ParseResult{
			err: NewInvalidDataError(result.err.Error()),
		}
	}
	p.tokens = p.unconsumedTokens
	p.unconsumedTokens = []Token{}
	p.tokens = append(p.tokens, result.Tokens...)

	parseResultRecord := []ParseResultRecord{}
	token := p.peek()
	startTokenIndex := p.index
	for token.tokenType != eofToken {

		astNode, err := p.parseRESPValue()

		if err != nil {
			if err.errorType == ParseErrorInvalidData {
				return ParseResult{
					err:     err,
					records: parseResultRecord,
				}
			} else if err.errorType == ParseErrorMissingChunk {
				p.unconsumedTokens = p.tokens[p.index:]
				return ParseResult{
					err:     nil,
					records: parseResultRecord,
				}
			}
		}
		fmt.Println("")
		parseResultRecord = append(parseResultRecord, ParseResultRecord{
			AstNode:  astNode,
			RawInput: p.getRawInput(startTokenIndex, p.index),
		})
		p.next()
		startTokenIndex = p.index
		p.skipWhiteSpaces()
		token = p.peek()

	}

	return ParseResult{
		err:     nil,
		records: parseResultRecord,
	}
}

func (p *Parser) parseInteger() (ASTNode, *ParseError) {
	numberTok := p.next()
	err := p.expect(numberToken)
	if err != nil {
		return nil, err
	}
	p.next()

	err = p.expect(CLRFToken)
	if err != nil {
		return nil, err
	}

	return ASTNumber{Val: numberTok.val.(int)}, nil
}

func (p *Parser) parseSimpleString() (ASTNode, *ParseError) {
	msgToken := p.next()
	err := p.expect(literalToken)
	if err != nil {
		return nil, err
	}

	p.next()
	err = p.expect(CLRFToken)
	if err != nil {
		return nil, err
	}

	return ASTSimpleString{Val: msgToken.val.(string)}, nil
}

func (p *Parser) parseSimpleError() (ASTNode, *ParseError) {

	token := p.next()
	err := p.expect(literalToken)
	if err != nil {
		return nil, err
	}
	p.next()

	literals := strings.Split(token.val.(string), " ")

	err = p.expect(CLRFToken)
	if err != nil {
		return nil, err
	}
	errorType := ""

	if isWordUpperCase(literals[0]) {
		errorType = literals[0]
		literals = literals[1:]
	}
	if literals[len(literals)-1] == "" {
		literals = literals[:len(literals)-1]
	}

	return ASTSimpleError{ErrType: errorType, Msg: strings.Join(literals, " ")}, nil
}

func (p *Parser) parseBulkString() (ASTNode, *ParseError) {
	lengthToken := p.next()
	if err := p.expect(numberToken); err != nil {
		return nil, err
	}

	p.next()
	err := p.expect(CLRFToken)
	if err != nil {
		return nil, err
	}
	// -1 special case, for null bulk token
	if lengthToken.val.(int) == -1 {
		token := p.next()
		err := p.expect(literalToken)
		if err != nil {
			return nil, err
		}
		if token.val != "" {
			return nil, NewInvalidDataError("Expected token to have null bulk string")
		}
		return ASTBulkString{Val: ""}, nil
	}

	token := p.next()
	err = p.expect(literalToken)
	if err != nil {
		return nil, err
	}
	data := token.val.(string)

	token = p.next()

	if len(data) != lengthToken.val.(int) {
		return nil, NewInvalidDataError(fmt.Sprintf("ParseBulkString error, length mismatch declared: %v got: %v bytes", len(data), lengthToken.val))
	}

	err = p.expect(CLRFToken)
	if err != nil {
		return nil, err
	}
	return ASTBulkString{Val: data}, nil
}

// Grammar (BNF Notation)
//
// Notes:
// - ()* means "zero or more occurrences"
// - | means "or" (alternative)
// - ε (epsilon) means "empty string" (matches nothing)
// - Terminals are tokens from the lexer (colonToken, literalToken, etc.)
// - Non-terminals are production rules (Integer, SimpleString, etc.)
//
// Syntactic Grammar (Parser Level):
// RESPValue    -> Integer | SimpleString | SimpleError | BulkString | ε
// Integer      -> colonToken numberToken CLRFToken
// SimpleString -> plusToken literalToken CLRFToken
// SimpleError  -> hyphenToken LiteralSeq CLRFToken
// BulkString   -> dollarToken numberToken CLRFToken LiteralSeq CLRFToken
// Array 	    -> starToken numberToken CLRF RESPValue
//
// LiteralSeq   -> literalToken (spaceToken literalToken)*
//
// ---------------------------------------------------------------
// Lexical Grammar (Lexer/Tokenizer Level - for reference only):
// literalToken -> alphaChar+ | digitChar+
// numberToken  -> digitChar+
// alphaChar    -> 'a'..'z' | 'A'..'Z'
// digitChar    -> '0'..'9'
func (p *Parser) praseArray() (ASTNode, *ParseError) {
	length := p.next()
	err := p.expect(numberToken)

	if err != nil {
		return nil, err
	}

	p.next()
	err = p.expect(CLRFToken)
	if err != nil {
		return nil, err
	}

	lengthNumber, ok := length.val.(int)
	fmt.Println(lengthNumber)
	if !ok {
		return nil, NewInvalidDataError("parseArray, couldn't parse length token val to number")
	}

	arrayAST := []ASTNode{}

	for i := 0; i < lengthNumber; i++ {
		p.next()
		ast, err := p.parseRESPValue()
		if err != nil {
			return nil, err
		}
		arrayAST = append(arrayAST, ast)
	}

	return ASTArray{Values: arrayAST}, nil
}

func (p *Parser) parseRESPValue() (ASTNode, *ParseError) {
	var ast ASTNode
	var err *ParseError

	switch p.peek().tokenType {
	case colonToken:
		ast, err = p.parseInteger()
	case plusToken:
		ast, err = p.parseSimpleString()
	case hyphenToken:
		ast, err = p.parseSimpleError()
	case dollarToken:
		ast, err = p.parseBulkString()
	case starToken:
		ast, err = p.praseArray()
	default:
		panic(fmt.Sprintf("Not supported token type: %v", p.peek().tokenType))
	}
	return ast, err
}

func NewParser() *Parser {
	return &Parser{
		index: 0,
	}
}
